{"cells":[{"cell_type":"markdown","source":["#Power Plant ML Pipeline Application\nThis is an end-to-end example of using a number of different machine learning algorithms to solve a supervised regression problem.\n\n###Table of Contents\n- *Step 1: Business Understanding*\n- *Step 2: Extract-Transform-Load (ETL) Your Data*\n- *Step 3: Explore Your Data*\n- *Step 4: Visualize Your Data*\n- *Step 5: Data Preparation*\n- *Step 6: Data Modeling*\n\n\n*We are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.  Power generation is a complex process, and understanding and predicting power output is an important element in managing a plant and its connection to the power grid.*\n\nMore information about Peaker or Peaking Power Plants can be found on Wikipedia https://en.wikipedia.org/wiki/Peaking_power_plant\n\n\nGiven this business problem, we need to translate it to a Machine Learning task.  The ML task is regression since the label (or target) we are trying to predict is numeric.\n\n\nThe example data is provided by UCI at [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)\n\nYou can read the background on the UCI page, but in summary we have collected a number of readings from sensors at a Gas Fired Power Plant\n\n(also called a Peaker Plant) and now we want to use those sensor readings to predict how much power the plant will generate.\n\n\nMore information about Machine Learning with Spark can be found in the programming guide in the [SparkML Guide](https://spark.apache.org/docs/latest/mllib-guide.html)\n\n\n*Please note this example only works with Spark version 1.4 or higher*"],"metadata":{}},{"cell_type":"code","source":["assert int(sc.version.replace(\".\", \"\")) >= 140, \"Spark 1.4.0+ is required to run this notebook. Please attach it to a Spark 1.4.0+ cluster.\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["##Step 1: Business Understanding\nThe first step in any machine learning task is to understand the business need. \n\nAs described in the overview we are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.\n\nThe problem is a regression problem since the label (or target) we are trying to predict is numeric"],"metadata":{}},{"cell_type":"markdown","source":["##Step 2: Extract-Transform-Load (ETL) Your Data\n\nNow that we understand what we are trying to do, the first step is to load our data into a format we can query and use.  This is known as ETL or \"Extract-Transform-Load\".  We will load our file from Amazon s3.\n\nNote: Alternatively we could upload our data using \"Databricks Menu > Tables > Create Table\", assuming we had the raw files on our local computer.\n\n%md Our data is available on Amazon s3 at the following path:  \n`dbfs:/databricks-datasets/power-plant/data`\n\n**ToDo:** Let's start by printing the first 5 lines of the file.  \n*Hint*: To read the file into an RDD use `sc.textFile(\"dbfs:/databricks-datasets/power-plant/data\")`  \n*Hint*: Then you will need to figure out how to `take` and print the first 5 lines of the RDD."],"metadata":{}},{"cell_type":"code","source":["rawTextRdd = sc.textFile(\"dbfs:/databricks-datasets/power-plant/data\")\nfor line in rawTextRdd.take(5):\n    print(line)\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["The file is a .tsv (Tab Seperated Values) file of floating point numbers.  \n\nOur schema definition from UCI appears below:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output.  This is the value we are trying to predict given the measurements above.\n\n\n**ToDo:** Transform the RDD so that each row is a tuple of float values.  Then print the first 5 rows.  \n*Hint:* Use filter to exclude lines that start with AT to remove the header.  \n*Hint:* Use map to transform each line into a PowerPlantRow of data fields.  \n*Hint:* Use python's str.split break up each line into individual fields."],"metadata":{}},{"cell_type":"code","source":["from collections import namedtuple\nPowerPlantRow=namedtuple(\"PowerPlantRow\", [\"AT\", \"V\", \"AP\", \"RH\", \"PE\"])\nrawDataRdd=rawTextRdd\\\n  .map(lambda x: x.split(\"\\t\"))\\\n  .filter(lambda line: line[0] != \"AT\")\\\n  .map(lambda line: PowerPlantRow(float(line[0]), float(line[1]), float(line[2]), float(line[3]), float(line[4])))\nrawDataRdd.take(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["##Step 3: Explore Your Data\nNow that your data is loaded, let's explore it, verify it, and do some basic analysis and visualizations."],"metadata":{}},{"cell_type":"markdown","source":["**ToDo:** Transform your `rawDataRdd` into a Dataframe named `power_plant`.  Then use the `display(power_plant)` function to visualize it."],"metadata":{}},{"cell_type":"code","source":["powerPlant=None"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Next, let's register our dataframe as an SQL table.  Because this lab may be run many times, we'll take the precaution of removing any existing tables first.\n\n**ToDo:** Execute the prepared code in the following cell..."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"DROP TABLE IF EXISTS power_plant\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/power_plant\", True)\nNone"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["**ToDo:** Register your `powerPlant` dataframe as the table named `power_plant`"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["**ToDo:** Perform the query `SELECT * FROM power_plant`"],"metadata":{}},{"cell_type":"code","source":["%sql "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["**ToDo:** Use the `desc power_plant` SQL command to describe the schema"],"metadata":{}},{"cell_type":"code","source":["%sql "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["**Schema Definition**\n\nOur schema definition from UCI appears below:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output\n\nPE is our label or target. This is the value we are trying to predict given the measurements.\n\n*Reference [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{}},{"cell_type":"markdown","source":["**ToDo:** Display summary statistics for the the columns.  \n*Hint:* To access the table from python use `sqlContext.table(\"power_plant\")`  \n*Hint:* We can use the describe function with no parameters to get some basic stats for each column like count, mean, max, min and standard deviation. The describe function is a method attached to a dataframe. More information can be found in the [Spark API docs](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["##Step 4: Visualize Your Data\n\nTo understand our data, we will look for correlations between features and the label.  This can be important when choosing a model.  E.g., if features and a label are linearly correlated, a linear model like Linear Regression can do well; if the relationship is very non-linear, more complex models such as Decision Trees can be better. We use Databrick's built in visualization to view each of our predictors in relation to the label column as a scatter plot to see the correlation between the predictors and the label."],"metadata":{}},{"cell_type":"markdown","source":["**ToDo:** Do a scatter plot of Power(PE) as a function of Temperature (AT).  \n*Bonus:* Name the y-axis \"Power\" and the x-axis \"Temperature\""],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Notice there appears to be a strong linear correlation between temperature and Power Output"],"metadata":{}},{"cell_type":"markdown","source":["**ToDo:** Do a scatter plot of Power(PE) as a function of ExhaustVacuum (V).  \n*Bonus:* Name the y-axis \"Power\" and the x-axis \"ExhaustVacuum\""],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["The linear correlation is not as strong between Exhaust Vacuum Speed and Power Output but there is some semblance of a pattern."],"metadata":{}},{"cell_type":"markdown","source":["**ToDo:** Do a scatter plot of Power(PE) as a function of Pressure (AP).  \n*Bonus:* Name the y-axis \"Power\" and the x-axis \"Pressure\""],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["**ToDo:** Do a scatter plot of Power(PE) as a function of Humidity (RH).  \n*Bonus:* Name the y-axis \"Power\" and the x-axis \"Humidity\""],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["...and atmospheric pressure and relative humidity seem to have little to no linear correlation"],"metadata":{}}],"metadata":{"name":"Power-Lab2-SQL","notebookId":9029},"nbformat":4,"nbformat_minor":0}
